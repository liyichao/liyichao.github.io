---
layout: post
title: "tracing 系统的 cost"
category: 
tags: []
---
{% include JB/setup %}

## 什么是 tracing
tracing 是 APM (Application Performance Management) 的一部分，它追踪每个页面请求、RPC 请求，从而让人知道每个页面请求，从发出请求到收到结果，中间的每段时间花费在哪里了。

* trace: 一次完整的追踪，对于主站，一次 URL 请求就是一个 trace
* span: 跨越一段时间的事件，一次 RPC 请求就是一个 span，一个 trace 就是一棵树，树里每个节点是一个 span。

## 一次灾难

不久前，我写了一个可以往 [zipkin]() 打 trace 数据的 python 库，并已在 `snow`，`wish` 框架里埋点，更新了 `kernelserver01` 上 kernel 服务依赖的 `wish` 版本号，测试了多天，没有可见的性能恶化。

9月24号 20:12，增加了 tracing 功能的主站（hot-v7.00-1.455）上线 alpha，看到了主站的 [trace](http://trace.in.zhihu.com/traces/007147922d8150e2)。

9月25号 15:11，该版本被陈铮带入 all。主站响应时间[急剧上升](https://files.slack.com/files-pri/T025MQ81Q-F02M99ECJ/9_25_3______5____web_p95.png)。

陈铮观察了一会，发现不对，赶紧于16:06回滚，主站于16:40左右恢复正常。陈铮以为是自己的上线导致的问题，苦思冥想，于 18:39 发布一个 fix 版本，发现不行，于 19:32 又发了一个 fix，观察了半小时，无奈，只得于 19:32 回滚。在9月26号 11:40，又发布了一个 fix，依然不行。

9月26号 13:00，成城老师介入，怀疑是 tracing 系统的问题，查看[Hero](http://hero.in.zhihu.com/#/records)，tracing 的引入的确是上一个版本，于是于 13:37 把主站 tracing 代码完全 revert，重新上线，主站于 13:40 [恢复正常](https://files.slack.com/files-pri/T025MQ81Q-F02M7TJ9T/9_26_11______15___.png)。

至此，确定是 tracing 系统导致的主站性能恶化。

## 原因

关键数据（python 函数的耗时数据在 gw 运行得到，脚本在 gw 的 `/home/liyichao/cost.py`）：

1. 主站 9月25号 15:00-17:00 QPS：[大于800](https://files.slack.com/files-pri/T025MQ81Q-F02M80LUX/9_25_15______17___qps.png)
2. 主站单一请求的RPC调用数量：很多是100+，我见到的最高的是 1021。其中基本都是两个接口，一个是[`answer_is_normal`](http://trace.in.zhihu.com/traces/00160a98acca32ee?serviceName=url)，另一个是[`comment.commentservice`](http://trace.in.zhihu.com/traces/000fe002c72863f9?serviceName=url)。(很可惜，灾难当天的 trace 已经被清掉了，只得以今天上线办公室后的 trace 数据)
3. `a = gevent.local.local().trace_id`：`2.5e-06`
4. `random.randint(0, 2**56 - 1)`：`8.2e-06`
5. `time.time()`：`8.2e-06`

响应时间急剧上升的原因：

* 最主要的原因是一个 RPC 请求调用一次`Kids.log()`。这个调用没有缓冲，也没有异步。Traces 通过 kids 收集，每次 RPC 请求结束，都会把 span 通过 `Kids.log()` 打出来。这意味着一个 URL 请求，无缓冲非异步的打 100+ 的 `log`。我注意到，虽然`snow`和`wish`的 server 端是会`log`每个请求，但是它们的客户端都是不会打`log`的。另外，由于采用 JSON 对 span 序列化，消息长度过长，`log` 阻塞的几率也增大。
* 一次 snow 或 wish 的 RPC 调用，tracing 的消耗是（只管上面提到的 3，4，5）：

    3=12次，4=1次，5=5次

    （以上计算针对的是出问题的主站版本 （hot-v7.00-1.455），其中依赖的 snow==1.0.12dev，wish==0.7.9dev，    当时的 tracing==0.0.21，欢迎校验我的计算的正确性。）
    按一个 URL 请求 300 个 RPC 算，总共消耗是：237.6 ms，还是挺可观的。

## 新进展

激动人心的时刻来了！tracing 0.0.23 的新功能如下：

* 性能改善：
    * `Kids.log()`异步+缓冲，对主进程的影响仅仅是把 span 放入队列，而这个操作也是在缓冲了`flush_interval` 个 span 后做，因此这个开销基本没有。
    * 不再使用 `gevent.local`，使用`threading.local`，目前 tracing 线程安全，但不支持跨线程的 RPC 调用的记录。
    * 使用 `random.getrandbits(53)`得到`uid`，这个速度大概为`1e-06`。
    * 重构 tracing，使得各 `record_*`可以接受可选的`trace_id`参数，3类调用的次数现在是: 3=1次，4=1次，5=2次。300个 span 的 URL 请求消耗：52 ms。

* 查看某类型的页面的 trace：由于 annotation 搜索无法用，之前不能筛选 trace，比如只查看问题页的 trace。现在通过选择 span name，选中 questionhandler，可以查看问题页的 trace，相当于

        span_name = Hander.__class__.__name__

* 支持 Firefox 插件。这意味着，现在，可以查看你的这次访问的 trace。装上这个插件，打开插件，访问`www.zhihu.com`，你就可以点击它里面的链接，看到你的这次访问的 trace。不多说了，[看图](https://files.slack.com/files-pri/T025MQ81Q-F02MK1A69/firefox.png)。

    插件下载请点[我](https://github.com/twitter/zipkin/tree/master/zipkin-browser-extension/firefox)，配置的两个地址分别是：
    
        site to trace: ^www\d*.zhihu.(dev|com)$
        zipkin URL: http://trace.in.zhihu.com
    